{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0875781",
   "metadata": {
    "id": "al6vQodt_CDw"
   },
   "source": [
    "### **Create the image folders for the corresponding classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a853204",
   "metadata": {
    "id": "8ZV4acQgZpLu"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/content/fingers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/fingers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m new_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/Assignment3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m subDirectories \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Image Dataset holder\u001b[39;00m\n\u001b[0;32m     13\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/fingers'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#import torch\n",
    "#Path of dataset and new dataset,change these two variables if needed \n",
    "path=\"C:/Users/vshar/OneDrive/Desktop/Finger/fingers\"\n",
    "new_path = \"C:/Users/vshar/OneDrive/Desktop/Assignment3\"\n",
    "\n",
    "subDirectories = os.listdir(path) \n",
    "\n",
    "#Image Dataset holder\n",
    "images = []\n",
    "imageList= []\n",
    "\n",
    "#from csv import writer\n",
    " \n",
    "\n",
    "hands=['L','R'] \n",
    "labels1=[]\n",
    "labels2={}\n",
    "\n",
    "for hand in hands:\n",
    "    for i in range(0,6):\n",
    "        labels1.append(str(i)+hand) \n",
    "        labels2[str(i)+hand]=str(i)+hand\n",
    "\n",
    "counterImagesProcessed=0\n",
    "if os.path.exists(new_path)==False:\n",
    "    os.mkdir(new_path)\n",
    "    for folder in subDirectories:\n",
    "      os.mkdir(new_path+\"/\"+folder)\n",
    "      for category in labels2:\n",
    "        os.mkdir(new_path+\"/\"+folder+\"/\"+category)\n",
    "    #newSubDirectories = os.listdir(new_path) \n",
    "    for folder in subDirectories:\n",
    "        currentPath=path+\"/\" +folder\n",
    "        images = os.listdir(currentPath)\n",
    "        for image in images:\n",
    "            newCurrentPath=new_path+\"/\"+folder+\"/\"+image[-6]+image[-5]+\"/\"+image\n",
    "            cv2.imwrite(newCurrentPath,cv2.imread(currentPath+\"/\"+image))\n",
    "            counterImagesProcessed=counterImagesProcessed+1\n",
    "\n",
    "else:\n",
    "    print(\"Dataset Alreay Exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d10581b",
   "metadata": {
    "id": "9__zlXs1_G6L"
   },
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092e860f",
   "metadata": {
    "id": "OTdQ_B7eZvjz"
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as td\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a66d97",
   "metadata": {
    "id": "AtU5YaFQDCsr"
   },
   "source": [
    "### **Set device to GPU or CPU. Also set dataset label map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d70e78",
   "metadata": {
    "id": "6yKJ3BqRZxW2"
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "labels_map = {\n",
    "      0: \"0-Left\",\n",
    "      1: \"0-Right\",\n",
    "      2: \"1-Left \",\n",
    "      3: \"1-Right \",\n",
    "      4: \"2-Left \",\n",
    "      5: \"2-Right \",\n",
    "      6: \"3-Left \",\n",
    "      7: \"3-Right \",\n",
    "      8: \"4-Left \",\n",
    "      9: \"4-Right\",\n",
    "      10: \"5-Left \",\n",
    "      11: \"5-Right \",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675149b",
   "metadata": {
    "id": "qszwtk-qDP4N"
   },
   "source": [
    "### **Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17bcc7ef",
   "metadata": {
    "id": "NVSxuWcnZzPD"
   },
   "outputs": [],
   "source": [
    "def data_loader(data_dir_input,batch_sizeGiven,input_size,test_split,val_split,flag=0):\n",
    "# Define dataset directory and transforms\n",
    "  data_dir = data_dir_input #\n",
    "  test_transform=train_transform = transforms.Compose([\n",
    "      transforms.Resize(input_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "      transforms.RandomRotation(50),\n",
    "      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "      transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "      transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "      transforms.ToTensor(),           \n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "\n",
    "# Define train, validation, and test dataset\n",
    "  train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train/'), transform=train_transform)\n",
    "  data_directory_test = data_dir+\"/\"+\"test\"+\"/\"\n",
    "  test_dataset = datasets.ImageFolder(data_directory_test, transform=test_transform)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True)\n",
    "  val_size = int(ceil((len(test_dataset)*val_split) / (test_split+val_split)))\n",
    "  test_size = len(test_dataset) - val_size\n",
    "  test_dataset, val_dataset = td.random_split(test_dataset, [test_size, val_size])\n",
    "\n",
    "\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True) #shuffle \n",
    "  val_loader = DataLoader(val_dataset, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True) # shuffle working\n",
    "  if flag==1:\n",
    "    print(\"Train Datset Size Before Split\",len(train_dataset))\n",
    "    print(\"Test Datset Size Before Split\",len(test_dataset))\n",
    "    print(\"################################################\")\n",
    "    print(\"Train Datset Size After Split\",len(train_dataset))\n",
    "    print(\"Test Datset Size After Split\",len(test_dataset))\n",
    "    print(\"Validation Datset Size After Split\",len(val_dataset))\n",
    "\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    cols, rows = 3, 3\n",
    "    for i in range(1, cols * rows + 1):\n",
    "      sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "      img, label = train_dataset[sample_idx]\n",
    "      img=img.view(128,128,3)\n",
    "      figure.add_subplot(rows, cols, i)\n",
    "      plt.title(labels_map[label])\n",
    "      plt.axis(\"off\")\n",
    "      plt.imshow(img.squeeze())\n",
    "    print(\"\\n\\n\\n################################################\\n\\n\\n\")\n",
    "    print(\"Dataset After Pre-Processing\")\n",
    "    plt.show()\n",
    "    return  \n",
    "\n",
    "\n",
    "\n",
    "  return train_loader,test_loader,val_loader\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede4fa1",
   "metadata": {
    "id": "Q2x5ptlFDwCV"
   },
   "source": [
    "### **Dataset Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfbb16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "EFdnntLAZ1lK",
    "outputId": "fc16c148-faa1-4413-9b17-5c4e78615767"
   },
   "outputs": [],
   "source": [
    "BeforeImage = image.imread(new_path+\"/test/0L/00ab429e-7edd-4cf7-b6a6-25eec65d5cda_0L.png\")\n",
    "plt.figure()\n",
    "plt.title(\"0 Left (Before Preprocessing)\")\n",
    "plt.imshow(BeforeImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c5462",
   "metadata": {
    "id": "WZMwg5ztD0WC"
   },
   "source": [
    "### **Preprocessed Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369caa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vWA-aPnHZ3YU",
    "outputId": "a1b02ff0-235a-4b43-d77d-6d779b720ebd"
   },
   "outputs": [],
   "source": [
    "def Display(data_dir_input,batch_sizeGiven,input_size,test_split,val_split,flag=1):\n",
    "  data_loader(data_dir_input,batch_sizeGiven,input_size,test_split,val_split,1)\n",
    "Display(\"C:/Users/vshar/OneDrive/Desktop/Finger/fingers\",32, (128, 128) ,0.2, 0.1,1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64095bd6",
   "metadata": {
    "id": "oLXJw6j4D2H1"
   },
   "source": [
    "### **Training Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbc741",
   "metadata": {
    "id": "G67lumBTZ5UK"
   },
   "outputs": [],
   "source": [
    "def train(num_epochsGiven, model, train_loader, criterion, optimizer,val_loader):\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(\"Device: {}\".format(device))\n",
    "  model.to(device)\n",
    "  Accuracy=[]\n",
    "  validAccuracy=[]\n",
    "  Loss = []\n",
    "  num_epochs = num_epochsGiven\n",
    "  total_steps = len(train_loader)\n",
    "  t1 = time.time()\n",
    "  total,correct,loss=0,0,0\n",
    "  for epoch in range(num_epochs):\n",
    "      stepAcc=[]\n",
    "      stepLoss=[]    \n",
    "      for i, data in enumerate(train_loader):\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          # Forward pass\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "          # Backprop and optimisation\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          # Train accuracy\n",
    "          total = labels.size(0)\n",
    "          _,predicted = torch.max(outputs.data, 1)\n",
    "          correct = (predicted == labels).sum().item()\n",
    "          stepAcc.append((correct / total) * 100)\n",
    "          stepLoss.append(loss.item())\n",
    "          if (i + 1) % 100 == 0:\n",
    "              print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_steps, loss.item(),\n",
    "                     (correct / total) * 100))\n",
    "      correct_v = 0\n",
    "      total_v = 0\n",
    "      for dataVal in val_loader:\n",
    "          images_v, labels_v = dataVal[0].to(device), dataVal[1].to(device)\n",
    "          outputs = model(images_v)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          correct_v += (predicted == labels_v).sum().item()\n",
    "          total_v += labels_v.size(0)        \n",
    "      Accuracy.append(sum(stepAcc)/len(stepAcc))\n",
    "      validAccuracy.append((correct_v / total_v) * 100)\n",
    "      Loss.append(sum(stepLoss)/len(stepLoss))\n",
    "  print(\"######## Training Finished in {} seconds ###########\".format(time.time()-t1))\n",
    "  return Loss,Accuracy,model,validAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8f0f7",
   "metadata": {
    "id": "XZK42K6MEPiE"
   },
   "source": [
    "### **Testing of Model using different metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e48a46",
   "metadata": {
    "id": "TRhBbUwnZ6Ic"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report\n",
    "def test(model, device, test_loader):\n",
    "  model.eval() \n",
    "  y_truth=[]\n",
    "  y_predicted=[]\n",
    "  cm=[]\n",
    "  with torch.no_grad(): \n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for data in test_loader:\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          #print(predicted,labels)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "          y_truth+=labels.cpu().numpy().tolist()\n",
    "          y_predicted+=predicted.cpu().numpy().tolist()\n",
    "      print('Test Accuracy of the model on the {} test images: {} %'.format(total, (correct / total) * 100))\n",
    "      cm= confusion_matrix(y_truth,y_predicted)\n",
    "  print(classification_report(y_truth,y_predicted))\n",
    "  return cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29999234",
   "metadata": {
    "id": "i5lgsutsEXi-"
   },
   "source": [
    "### **Model (ResNet18)(ImageNet Version1 )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201a279",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "M5jQX824Z8x5",
    "outputId": "a2491139-10dc-4d36-ea83-ed7c3636d3a7"
   },
   "outputs": [],
   "source": [
    "MODELS3=[]\n",
    "hyper_parameters3 = []\n",
    "train_acc_hyper_paramaters3=[]\n",
    "train_acc_valid_hyper_paramaters3=[]\n",
    "train_loss_hyper_paramaters3=[]\n",
    "\n",
    "stringCrit=\"Cross Entropy Loss\"\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "batch_sizes=[32,64]\n",
    "learning_rates=[0.001,.0001]\n",
    "epochs=5\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "  for batch_size in batch_sizes:\n",
    "    for optimizer_count in range(2):\n",
    "      model3 = models.resnet18(weights=\"ResNet18_Weights.IMAGENET1K_V1\",progress=False)\n",
    "      model3.fc = nn.Linear(512, 12)\n",
    "      stringOPTM=\"\"\n",
    "      if optimizer_count%2==0:\n",
    "       stringOPTM=\"Adam\" \n",
    "       optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)\n",
    "      else:\n",
    "        stringOPTM=\"SGD\"\n",
    "        optimizer = torch.optim.SGD(model3.parameters(), lr=learning_rate, momentum=0.9)\n",
    "      hyper_parameters3.append([\"Learning Rate: \"+str(learning_rate)+\" Batch Size: \"+str(batch_size),stringCrit,stringOPTM])\n",
    "      print(\"Learning Rate: \"+str(learning_rate)+\" Batch Size: \"+str(batch_size),\" Loss function: \",stringCrit,\" Optimizer: \",stringOPTM)\n",
    "      #optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "      train_loader, test_loader, val_loader = data_loader(\"/content/Assignment3\",batch_size, (224, 224) ,0.2, 0.1 )\n",
    "      tempLoss,tempAccuracy,tempModel,tempValidAccuracy = train(epochs,model3,train_loader,criterion,optimizer,val_loader)\n",
    "      train_loss_hyper_paramaters3.append(tempLoss)\n",
    "      train_acc_hyper_paramaters3.append(tempAccuracy)\n",
    "      train_acc_valid_hyper_paramaters3.append(tempValidAccuracy)\n",
    "      cmReturned = test(model3,\"cuda\",test_loader)\n",
    "      if cmReturned is not None:\n",
    "          fig, ax = plt.subplots(figsize=(7, 7))\n",
    "          ConfusionMatrixDisplay(cmReturned).plot(ax=ax,cmap='Blues', xticks_rotation='vertical', values_format='d')\n",
    "          plt.show()\n",
    "      torch.save(tempModel,'/content/drive/MyDrive/Models Resnet TF/'+\"_\".join([\"Learning Rate: \"+str(learning_rate)+\" Batch Size: \"+str(batch_size),stringCrit,stringOPTM])+\".pth\")\n",
    "      torch.save(tempModel,'/content/drive/MyDrive/Models Resnet TF/'+\"_\".join([\"Learning Rate: \"+str(learning_rate)+\" Batch Size: \"+str(batch_size),stringCrit,stringOPTM])+\".pt\")     \n",
    "      MODELS3.append(tempModel)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bbd45",
   "metadata": {
    "id": "j4HCVLznE2IL"
   },
   "source": [
    "### **Accuracy Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7295624",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pxCmqxpSaSDY",
    "outputId": "54b3650e-8d3b-49f9-d220-d4d3ff1de9df"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from numpy.core.multiarray import dtype\n",
    "####### plot the the training accuracy here #########\n",
    "num_epochs=5\n",
    "epochs = [i for i in range(num_epochs)]\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy plot')\n",
    "\n",
    "#train_loss_hyper_paramaters.numpy()\n",
    "#print(train_loss_hyper_paramaters[\"lr0.01\"])\n",
    "\n",
    "for i in range(len(hyper_parameters3)):\n",
    "    plt.plot(train_acc_hyper_paramaters3[i],label=hyper_parameters3[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16091202",
   "metadata": {
    "id": "eg74ZoILE52U"
   },
   "source": [
    "### **Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4b9db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8azZdUZjaT31",
    "outputId": "ebc97606-7b23-42e8-82e6-8f4775b36b24"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from numpy.core.multiarray import dtype\n",
    "####### plot the the training loss here #########\n",
    "num_epochs=5\n",
    "epochs = [i for i in range(num_epochs)]\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss plot ResNet18')\n",
    "\n",
    "#train_loss_hyper_paramaters.numpy()\n",
    "#print(train_loss_hyper_paramaters[\"lr0.01\"])\n",
    "\n",
    "for i in range(len(hyper_parameters3)):\n",
    "    plt.plot(train_loss_hyper_paramaters3[i],label=hyper_parameters3[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40db4d4",
   "metadata": {
    "id": "7cf_UdsXFKF2"
   },
   "source": [
    "### **Validation Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5df0ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8LuCcBgSCMZl",
    "outputId": "d3019ddd-404d-471e-c223-1b5bc21f119d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from numpy.core.multiarray import dtype\n",
    "####### plot the the training loss here #########\n",
    "num_epochs=5\n",
    "epochs = [i for i in range(num_epochs)]\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation plot ResNet18')\n",
    "\n",
    "#train_loss_hyper_paramaters.numpy()\n",
    "#print(train_loss_hyper_paramaters[\"lr0.01\"])\n",
    "\n",
    "for i in range(len(hyper_parameters3)):\n",
    "    plt.plot(train_acc_valid_hyper_paramaters3[i],label=hyper_parameters3[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba0bf3",
   "metadata": {
    "id": "-CoGYYajFQ8x"
   },
   "source": [
    "### **Input image to the trained models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20775b84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTIDpKY5FPgN",
    "outputId": "9629dccb-232b-40af-b992-ab50c1d54ad9"
   },
   "outputs": [],
   "source": [
    "TempTransformer = transforms.Compose([\n",
    "      transforms.Resize((128,128), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "      transforms.RandomRotation(50),\n",
    "      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "      transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "      transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "      transforms.ToTensor(),           \n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "imageInput = Image.open(new_path+\"/train/5L/009850f5-6bf5-445d-88dd-25f6ccbe538b_5L.png\")\n",
    "input_data = TempTransformer(imageInput)\n",
    "input_data = input_data.to(device)\n",
    "input_data = input_data.unsqueeze(0)\n",
    "i=0\n",
    "for tempModelList in MODELS3:\n",
    "  tempModelList.eval()\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  tempModelList.to(device)\n",
    "  print(\" \".join(hyper_parameters3[i]))\n",
    "  with torch.no_grad():\n",
    "    output = tempModelList(input_data)\n",
    "\n",
    "  out=output.tolist()[0]\n",
    "  print(labels_map[out.index(max(out))])\n",
    "  i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82c5c0",
   "metadata": {
    "id": "iOG9nJNk-75J"
   },
   "source": [
    "### **Unzip the dataset,downloaded from kaggle on google colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c49e5c9",
   "metadata": {
    "id": "K5twXDmtYtEE"
   },
   "outputs": [],
   "source": [
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef356b",
   "metadata": {
    "id": "al6vQodt_CDw"
   },
   "source": [
    "### **Create the image folders for the corresponding classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbebe2",
   "metadata": {
    "id": "8ZV4acQgZpLu"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#import torch\n",
    "#Path of dataset and new dataset,change these two variables if needed \n",
    "path=\"/content/fingers\"\n",
    "new_path = \"/content/Assignment3\"\n",
    "\n",
    "subDirectories = os.listdir(path) \n",
    "\n",
    "#Image Dataset holder\n",
    "images = []\n",
    "imageList= []\n",
    "\n",
    "#from csv import writer\n",
    " \n",
    "\n",
    "hands=['L','R'] \n",
    "labels1=[]\n",
    "labels2={}\n",
    "\n",
    "for hand in hands:\n",
    "    for i in range(0,6):\n",
    "        labels1.append(str(i)+hand) \n",
    "        labels2[str(i)+hand]=str(i)+hand\n",
    "\n",
    "counterImagesProcessed=0\n",
    "if os.path.exists(new_path)==False:\n",
    "    os.mkdir(new_path)\n",
    "    for folder in subDirectories:\n",
    "      os.mkdir(new_path+\"/\"+folder)\n",
    "      for category in labels2:\n",
    "        os.mkdir(new_path+\"/\"+folder+\"/\"+category)\n",
    "    #newSubDirectories = os.listdir(new_path) \n",
    "    for folder in subDirectories:\n",
    "        currentPath=path+\"/\" +folder\n",
    "        images = os.listdir(currentPath)\n",
    "        for image in images:\n",
    "            newCurrentPath=new_path+\"/\"+folder+\"/\"+image[-6]+image[-5]+\"/\"+image\n",
    "            cv2.imwrite(newCurrentPath,cv2.imread(currentPath+\"/\"+image))\n",
    "            counterImagesProcessed=counterImagesProcessed+1\n",
    "\n",
    "else:\n",
    "    print(\"Dataset Alreay Exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60af703",
   "metadata": {
    "id": "9__zlXs1_G6L"
   },
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a934ab",
   "metadata": {
    "id": "OTdQ_B7eZvjz"
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as td\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a200a4",
   "metadata": {
    "id": "AtU5YaFQDCsr"
   },
   "source": [
    "### **Set device to GPU or CPU. Also set dataset label map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006044d",
   "metadata": {
    "id": "6yKJ3BqRZxW2"
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "labels_map = {\n",
    "      0: \"0-Left\",\n",
    "      1: \"0-Right\",\n",
    "      2: \"1-Left \",\n",
    "      3: \"1-Right \",\n",
    "      4: \"2-Left \",\n",
    "      5: \"2-Right \",\n",
    "      6: \"3-Left \",\n",
    "      7: \"3-Right \",\n",
    "      8: \"4-Left \",\n",
    "      9: \"4-Right\",\n",
    "      10: \"5-Left \",\n",
    "      11: \"5-Right \",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4a3e4",
   "metadata": {
    "id": "qszwtk-qDP4N"
   },
   "source": [
    "### **Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1657a",
   "metadata": {
    "id": "NVSxuWcnZzPD"
   },
   "outputs": [],
   "source": [
    "def data_loader(data_dir_input,batch_sizeGiven,input_size,test_split,val_split,flag=0):\n",
    "# Define dataset directory and transforms\n",
    "  data_dir = data_dir_input #\n",
    "  test_transform=train_transform = transforms.Compose([\n",
    "      transforms.Resize(input_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "      transforms.RandomRotation(50),\n",
    "      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "      transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "      transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "      transforms.ToTensor(),           \n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "\n",
    "# Define train, validation, and test dataset\n",
    "  train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train/'), transform=train_transform)\n",
    "  data_directory_test = data_dir+\"/\"+\"test\"+\"/\"\n",
    "  test_dataset = datasets.ImageFolder(data_directory_test, transform=test_transform)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True)\n",
    "  val_size = int(ceil((len(test_dataset)*val_split) / (test_split+val_split)))\n",
    "  test_size = len(test_dataset) - val_size\n",
    "  test_dataset, val_dataset = td.random_split(test_dataset, [test_size, val_size])\n",
    "\n",
    "\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True) #shuffle \n",
    "  val_loader = DataLoader(val_dataset, batch_size=batch_sizeGiven, shuffle=True, drop_last=False, num_workers=0,pin_memory=True) # shuffle working\n",
    "  if flag==1:\n",
    "    print(\"Train Datset Size Before Split\",len(train_dataset))\n",
    "    print(\"Test Datset Size Before Split\",len(test_dataset))\n",
    "    print(\"################################################\")\n",
    "    print(\"Train Datset Size After Split\",len(train_dataset))\n",
    "    print(\"Test Datset Size After Split\",len(test_dataset))\n",
    "    print(\"Validation Datset Size After Split\",len(val_dataset))\n",
    "\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    cols, rows = 3, 3\n",
    "    for i in range(1, cols * rows + 1):\n",
    "      sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "      img, label = train_dataset[sample_idx]\n",
    "      img=img.view(128,128,3)\n",
    "      figure.add_subplot(rows, cols, i)\n",
    "      plt.title(labels_map[label])\n",
    "      plt.axis(\"off\")\n",
    "      plt.imshow(img.squeeze())\n",
    "    print(\"\\n\\n\\n################################################\\n\\n\\n\")\n",
    "    print(\"Dataset After Pre-Processing\")\n",
    "    plt.show()\n",
    "    return  \n",
    "\n",
    "\n",
    "\n",
    "  return train_loader,test_loader,val_loader\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55df43",
   "metadata": {
    "id": "Q2x5ptlFDwCV"
   },
   "source": [
    "### **Dataset Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fc81b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "EFdnntLAZ1lK",
    "outputId": "fc16c148-faa1-4413-9b17-5c4e78615767"
   },
   "outputs": [],
   "source": [
    "BeforeImage = image.imread(new_path+\"/test/0L/00ab429e-7edd-4cf7-b6a6-25eec65d5cda_0L.png\")\n",
    "plt.figure()\n",
    "plt.title(\"0 Left (Before Preprocessing)\")\n",
    "plt.imshow(BeforeImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aaf73a",
   "metadata": {
    "id": "WZMwg5ztD0WC"
   },
   "source": [
    "### **Preprocessed Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb804b0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vWA-aPnHZ3YU",
    "outputId": "a1b02ff0-235a-4b43-d77d-6d779b720ebd"
   },
   "outputs": [],
   "source": [
    "def Display(data_dir_input,batch_sizeGiven,input_size,test_split,val_split,flag=1):\n",
    "  data_loader(data_dir_input,batch_sizeGiven,input_size,test_split,val_split,1)\n",
    "Display(\"/content/Assignment3\",32, (128, 128) ,0.2, 0.1,1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d0aa1",
   "metadata": {
    "id": "oLXJw6j4D2H1"
   },
   "source": [
    "### **Training Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53ca26",
   "metadata": {
    "id": "G67lumBTZ5UK"
   },
   "outputs": [],
   "source": [
    "def train(num_epochsGiven, model, train_loader, criterion, optimizer,val_loader):\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(\"Device: {}\".format(device))\n",
    "  model.to(device)\n",
    "  Accuracy=[]\n",
    "  validAccuracy=[]\n",
    "  Loss = []\n",
    "  num_epochs = num_epochsGiven\n",
    "  total_steps = len(train_loader)\n",
    "  t1 = time.time()\n",
    "  total,correct,loss=0,0,0\n",
    "  for epoch in range(num_epochs):\n",
    "      stepAcc=[]\n",
    "      stepLoss=[]    \n",
    "      for i, data in enumerate(train_loader):\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          # Forward pass\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "          # Backprop and optimisation\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          # Train accuracy\n",
    "          total = labels.size(0)\n",
    "          _,predicted = torch.max(outputs.data, 1)\n",
    "          correct = (predicted == labels).sum().item()\n",
    "          stepAcc.append((correct / total) * 100)\n",
    "          stepLoss.append(loss.item())\n",
    "          if (i + 1) % 100 == 0:\n",
    "              print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_steps, loss.item(),\n",
    "                     (correct / total) * 100))\n",
    "      correct_v = 0\n",
    "      total_v = 0\n",
    "      for dataVal in val_loader:\n",
    "          images_v, labels_v = dataVal[0].to(device), dataVal[1].to(device)\n",
    "          outputs = model(images_v)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          correct_v += (predicted == labels_v).sum().item()\n",
    "          total_v += labels_v.size(0)        \n",
    "      Accuracy.append(sum(stepAcc)/len(stepAcc))\n",
    "      validAccuracy.append((correct_v / total_v) * 100)\n",
    "      Loss.append(sum(stepLoss)/len(stepLoss))\n",
    "  print(\"######## Training Finished in {} seconds ###########\".format(time.time()-t1))\n",
    "  return Loss,Accuracy,model,validAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2d70aa",
   "metadata": {
    "id": "XZK42K6MEPiE"
   },
   "source": [
    "### **Testing of Model using different metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d03f3",
   "metadata": {
    "id": "TRhBbUwnZ6Ic"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report\n",
    "def test(model, device, test_loader):\n",
    "  model.eval() \n",
    "  y_truth=[]\n",
    "  y_predicted=[]\n",
    "  cm=[]\n",
    "  with torch.no_grad(): \n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for data in test_loader:\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          #print(predicted,labels)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "          y_truth+=labels.cpu().numpy().tolist()\n",
    "          y_predicted+=predicted.cpu().numpy().tolist()\n",
    "      print('Test Accuracy of the model on the {} test images: {} %'.format(total, (correct / total) * 100))\n",
    "      cm= confusion_matrix(y_truth,y_predicted)\n",
    "  print(classification_report(y_truth,y_predicted))\n",
    "  return cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9692cd",
   "metadata": {
    "id": "i5lgsutsEXi-"
   },
   "source": [
    "### **Model (ResNet18)(ImageNet Version1 )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3c3ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "M5jQX824Z8x5",
    "outputId": "a2491139-10dc-4d36-ea83-ed7c3636d3a7"
   },
   "outputs": [],
   "source": [
    "MODELS3=[]\n",
    "hyper_parameters3 = []\n",
    "train_acc_hyper_paramaters3=[]\n",
    "train_acc_valid_hyper_paramaters3=[]\n",
    "train_loss_hyper_paramaters3=[]\n",
    "\n",
    "stringCrit=\"Cross Entropy Loss\"\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "batch_sizes=[32,64]\n",
    "learning_rates=[0.001,.0001]\n",
    "epochs=5\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "  for batch_size in batch_sizes:\n",
    "    for optimizer_count in range(2):\n",
    "      model3 = models.resnet18(weights=\"ResNet18_Weights.IMAGENET1K_V1\",progress=False)\n",
    "      model3.fc = nn.Linear(512, 12)\n",
    "      stringOPTM=\"\"\n",
    "      if optimizer_count%2==0:\n",
    "       stringOPTM=\"Adam\" \n",
    "       optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)\n",
    "      else:\n",
    "        stringOPTM=\"SGD\"\n",
    "        optimizer = torch.optim.SGD(model3.parameters(), lr=learning_rate, momentum=0.9)\n",
    "      hyper_parameters3.append([\"Learning Rate: \"+str(learning_rate)+\" Batch Size: \"+str(batch_size),stringCrit,stringOPTM])\n",
    "      print(\"Learning Rate: \"+str(learning_rate)+\" Batch Size: \"+str(batch_size),\" Loss function: \",stringCrit,\" Optimizer: \",stringOPTM)\n",
    "      #optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "      train_loader, test_loader, val_loader = data_loader(\"/content/Assignment3\",batch_size, (224, 224) ,0.2, 0.1 )\n",
    "      tempLoss,tempAccuracy,tempModel,tempValidAccuracy = train(epochs,model3,train_loader,criterion,optimizer,val_loader)\n",
    "      train_loss_hyper_paramaters3.append(tempLoss)\n",
    "      train_acc_hyper_paramaters3.append(tempAccuracy)\n",
    "      train_acc_valid_hyper_paramaters3.append(tempValidAccuracy)\n",
    "      cmReturned = test(model3,\"cuda\",test_loader)\n",
    "      if cmReturned is not None:\n",
    "          fig, ax = plt.subplots(figsize=(7, 7))\n",
    "          ConfusionMatrixDisplay(cmReturned).plot(ax=ax,cmap='Blues', xticks_rotation='vertical', values_format='d')\n",
    "          plt.show()\n",
    "      torch.save(tempModel,'/content/drive/MyDrive/Models Resnet TF/'+\"_\".join([\"Learning Rate: \"+str(learning_rate)+\" Batch Size: \"+str(batch_size),stringCrit,stringOPTM])+\".pth\")\n",
    "      torch.save(tempModel,'/content/drive/MyDrive/Models Resnet TF/'+\"_\".join([\"Learning Rate: \"+str(learning_rate)+\" Batch Size: \"+str(batch_size),stringCrit,stringOPTM])+\".pt\")     \n",
    "      MODELS3.append(tempModel)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c08f43",
   "metadata": {
    "id": "j4HCVLznE2IL"
   },
   "source": [
    "### **Accuracy Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed0091",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pxCmqxpSaSDY",
    "outputId": "54b3650e-8d3b-49f9-d220-d4d3ff1de9df"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from numpy.core.multiarray import dtype\n",
    "####### plot the the training accuracy here #########\n",
    "num_epochs=5\n",
    "epochs = [i for i in range(num_epochs)]\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy plot')\n",
    "\n",
    "#train_loss_hyper_paramaters.numpy()\n",
    "#print(train_loss_hyper_paramaters[\"lr0.01\"])\n",
    "\n",
    "for i in range(len(hyper_parameters3)):\n",
    "    plt.plot(train_acc_hyper_paramaters3[i],label=hyper_parameters3[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a99d7",
   "metadata": {
    "id": "eg74ZoILE52U"
   },
   "source": [
    "### **Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3beca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8azZdUZjaT31",
    "outputId": "ebc97606-7b23-42e8-82e6-8f4775b36b24"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from numpy.core.multiarray import dtype\n",
    "####### plot the the training loss here #########\n",
    "num_epochs=5\n",
    "epochs = [i for i in range(num_epochs)]\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss plot ResNet18')\n",
    "\n",
    "#train_loss_hyper_paramaters.numpy()\n",
    "#print(train_loss_hyper_paramaters[\"lr0.01\"])\n",
    "\n",
    "for i in range(len(hyper_parameters3)):\n",
    "    plt.plot(train_loss_hyper_paramaters3[i],label=hyper_parameters3[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc99f8d",
   "metadata": {
    "id": "7cf_UdsXFKF2"
   },
   "source": [
    "### **Validation Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e29c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8LuCcBgSCMZl",
    "outputId": "d3019ddd-404d-471e-c223-1b5bc21f119d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from numpy.core.multiarray import dtype\n",
    "####### plot the the training loss here #########\n",
    "num_epochs=5\n",
    "epochs = [i for i in range(num_epochs)]\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation plot ResNet18')\n",
    "\n",
    "#train_loss_hyper_paramaters.numpy()\n",
    "#print(train_loss_hyper_paramaters[\"lr0.01\"])\n",
    "\n",
    "for i in range(len(hyper_parameters3)):\n",
    "    plt.plot(train_acc_valid_hyper_paramaters3[i],label=hyper_parameters3[i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39b203",
   "metadata": {
    "id": "-CoGYYajFQ8x"
   },
   "source": [
    "### **Input image to the trained models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a1b79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTIDpKY5FPgN",
    "outputId": "9629dccb-232b-40af-b992-ab50c1d54ad9"
   },
   "outputs": [],
   "source": [
    "TempTransformer = transforms.Compose([\n",
    "      transforms.Resize((128,128), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "      transforms.RandomRotation(50),\n",
    "      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "      transforms.RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "      transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "      transforms.ToTensor(),           \n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "imageInput = Image.open(new_path+\"/train/5L/009850f5-6bf5-445d-88dd-25f6ccbe538b_5L.png\")\n",
    "input_data = TempTransformer(imageInput)\n",
    "input_data = input_data.to(device)\n",
    "input_data = input_data.unsqueeze(0)\n",
    "i=0\n",
    "for tempModelList in MODELS3:\n",
    "  tempModelList.eval()\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  tempModelList.to(device)\n",
    "  print(\" \".join(hyper_parameters3[i]))\n",
    "  with torch.no_grad():\n",
    "    output = tempModelList(input_data)\n",
    "\n",
    "  out=output.tolist()[0]\n",
    "  print(labels_map[out.index(max(out))])\n",
    "  i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efc33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
